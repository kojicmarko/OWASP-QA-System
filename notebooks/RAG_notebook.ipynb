{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2d8f75f",
   "metadata": {},
   "source": [
    "#### Prerequisites\n",
    " - Ollama installed\n",
    "\n",
    "##### Dependencies:\n",
    "  - \"unstructured[pdf]\"\n",
    "  - langchain\n",
    "  - langchain-community\n",
    "  - langchain-huggingface\n",
    "  - langchain-ollama\n",
    "  - langchain-chroma\n",
    "  - chromadb\n",
    "  - sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7455e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d05127-112c-423f-88cc-0ada05efae9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_ollama import OllamaLLM\n",
    "from transformers import AutoTokenizer\n",
    "from unstructured.documents.elements import ListItem, NarrativeText, Title, Element\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "notebook_path = Path().resolve()\n",
    "\n",
    "print(\"Imports ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e77e4-75d8-47d2-a9db-ced6ee28a8d4",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "506bee3b-2a39-430d-a84f-cb37cf2cfdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n",
      "Data-loss while decompressing corrupted data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "592\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"documents\"\n",
    "FILE_NAME = \"OWASP-Top-10-for-LLMs-2025.pdf\"\n",
    "START_MARKER = \"LLM01:2025\"\n",
    "END_MARKER = \"Appendix 1\"\n",
    "\n",
    "elements = partition_pdf(filename=f\"{notebook_path.parent}/{FILE_PATH}/{FILE_NAME}\")\n",
    "\n",
    "start_index = None\n",
    "end_index = None\n",
    "for i, e in enumerate(elements):\n",
    "    if START_MARKER in e.text.strip() and isinstance(e, Title):\n",
    "        start_index = i\n",
    "\n",
    "    if END_MARKER in e.text.strip() and isinstance(e, Title):\n",
    "        end_index = i\n",
    "        break\n",
    "\n",
    "elements_after_start = elements[start_index:end_index + 1]\n",
    "\n",
    "content_elements = [\n",
    "    e for e in elements[start_index:end_index]\n",
    "    if isinstance(e, (Title, NarrativeText, ListItem, Table))\n",
    "]\n",
    "\n",
    "print(len(content_elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17145f0e",
   "metadata": {},
   "source": [
    "### Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd6fc696-cca7-41f0-aff1-739b664cb746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created 109 chunks with hierarchical metadata.\n"
     ]
    }
   ],
   "source": [
    "SKIP_FILLER = \"OWASP Top 10 for LLM Applications v2.0\"\n",
    "JUNK_SECTION_TITLES = {\n",
    "    \"Reference Links\",\n",
    "    \"Related Frameworks and Taxonomies\",\n",
    "}\n",
    "\n",
    "chunks = []\n",
    "current_chunk_text = []\n",
    "section_context = \"\"\n",
    "current_heading = \"\"\n",
    "is_skipping_section = False\n",
    "\n",
    "for e in content_elements:\n",
    "    text = e.text.strip()\n",
    "    if not text or text == SKIP_FILLER:\n",
    "        continue\n",
    "\n",
    "    if isinstance(e, Title):\n",
    "        if section_context and text != section_context:\n",
    "            if len(current_chunk_text) > 1:\n",
    "                 metadata = {\"section\": section_context, \"heading\": current_heading, \"source\": f\"{FILE_NAME}\"}\n",
    "                 chunks.append({\"page_content\": \"\\n\".join(current_chunk_text), \"metadata\": metadata})\n",
    "                 current_chunk_text = []\n",
    "\n",
    "        if text in JUNK_SECTION_TITLES:\n",
    "            is_skipping_section = True\n",
    "            continue\n",
    "        \n",
    "        is_skipping_section = False\n",
    "        \n",
    "        if re.match(r\"^LLM\\d{2}:2025\", text):\n",
    "            section_context = text\n",
    "        \n",
    "        if not current_chunk_text:\n",
    "            current_heading = text\n",
    "            current_chunk_text.append(text)\n",
    "\n",
    "    elif not is_skipping_section:\n",
    "        current_chunk_text.append(text)\n",
    "\n",
    "if current_chunk_text and not is_skipping_section:\n",
    "    metadata = {\"section\": section_context, \"heading\": current_heading, \"source\": f\"{FILE_NAME}\"}\n",
    "    chunks.append({\"page_content\": \"\\n\".join(current_chunk_text), \"metadata\": metadata})\n",
    "\n",
    "\n",
    "print(f\"✓ Created {len(chunks)} chunks with hierarchical metadata.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34af11c4-3148-4c30-a948-bfca0eff3c84",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62ce2c6-b9d0-4131-b567-8cc578f115ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original chunks: 109\n",
      "RAG-ready sub-chunks: 111\n",
      "Max tokens in any RAG-ready chunk: 512\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "def split_by_tokens(text, max_tokens=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits a single string of text into smaller strings based on token count.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "        \n",
    "    result = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        chunk_text = tokenizer.decode(chunk_tokens)\n",
    "        result.append(chunk_text)\n",
    "        start += max_tokens - overlap\n",
    "\n",
    "    return result\n",
    "\n",
    "rag_ready_chunks = []\n",
    "\n",
    "for ch in chunks:\n",
    "    original_text = ch['page_content']\n",
    "    metadata = ch['metadata']\n",
    "    \n",
    "    sub_texts = split_by_tokens(original_text, max_tokens=512, overlap=50)\n",
    "    \n",
    "    for sub_text in sub_texts:\n",
    "        rag_ready_chunks.append({\n",
    "            \"page_content\": sub_text,\n",
    "            \"metadata\": metadata\n",
    "        })\n",
    "\n",
    "print(f\"Original chunks: {len(chunks)}\")\n",
    "print(f\"RAG-ready sub-chunks: {len(rag_ready_chunks)}\")\n",
    "\n",
    "if rag_ready_chunks:\n",
    "    max_len = max(len(tokenizer.encode(c['page_content'], add_special_tokens=False)) for c in rag_ready_chunks)\n",
    "    print(f\"Max tokens in any RAG-ready chunk: {max_len}\")\n",
    "else:\n",
    "    print(\"No RAG-ready chunks were created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c157733a-a37d-4eb3-b559-d789a0345162",
   "metadata": {},
   "source": [
    "### Embed & Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f74f915e-558f-45d3-90a1-3330e55a5b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Vector store ready!\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "documents = [\n",
    "    Document(page_content=chunk['page_content'], metadata=chunk['metadata'])\n",
    "    for chunk in rag_ready_chunks\n",
    "]\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"owasp_db_v1\",\n",
    "    # Optional: To save to disk, specify a directory\n",
    "    persist_directory=\"./chroma_db_v1\" \n",
    ")\n",
    "print(\"✓ Vector store ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6018b-9e1c-4042-909a-e84902edc78e",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f52691f8-67c4-4342-a492-405367269149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model ready: lama3.2:1b (via Ollama)\n",
      "✓ Model test: I'm ready to work.\n"
     ]
    }
   ],
   "source": [
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2:1b\",\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "print(\"✓ Model ready: lama3.2:1b (via Ollama)\")\n",
    "test_response = llm.invoke(\"Say 'ready' if you're working\")\n",
    "print(f\"✓ Model test: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9d6fa-a599-4fff-bb96-8ba54670fcdf",
   "metadata": {},
   "source": [
    "### QA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "918139df-6e61-4b6a-b074-df90d9855e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert assistant for answering questions about the OWASP Top 10 for LLM Applications document.\n",
    "    Use only the following retrieved context to answer the question.\n",
    "    If you don't know the answer from the context provided, just say that you do not have enough information to answer.\n",
    "    Be concise and directly answer the question.\n",
    "    \n",
    "    CONTEXT: {context}\n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Provide a concise security-focused answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22931019-aa1f-4d94-b84c-5198415fba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str, qa_chain):\n",
    "    \"\"\"\n",
    "    Asks a question using the QA chain and prints the answer along with the source documents.\n",
    "    \"\"\"\n",
    "    print(f\"-> QUESTION: {question}\\n\")\n",
    "    \n",
    "    response = qa_chain.invoke({\"query\": question})\n",
    "    \n",
    "    print(\"ANSWER:\")\n",
    "    print(\"============================================================\")\n",
    "    # Use textwrap to make the answer readable\n",
    "    print(textwrap.fill(response['result'], width=80))\n",
    "    print(\"============================================================\\n\")\n",
    "    \n",
    "    print(\"SOURCES:\")\n",
    "    print(\"============================================================\")\n",
    "    print(f\"Number of sources: {len(response['source_documents'])}\")\n",
    "    \n",
    "    for i, source in enumerate(response['source_documents']):\n",
    "        print(f\"\\n--- Source {i+1} ---\")\n",
    "        \n",
    "        metadata = source.metadata\n",
    "        print(f\"  SECTION: {metadata.get('section', 'N/A')}\")\n",
    "        print(f\"  HEADING: {metadata.get('heading', 'N/A')}\")\n",
    "        \n",
    "        print(\"\\n  CONTENT:\")\n",
    "        print(textwrap.fill(source.page_content, width=78, initial_indent=\"  \", subsequent_indent=\"  \"))\n",
    "        print(\"--------------------\")\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8092d8",
   "metadata": {},
   "source": [
    "### Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44ab351e-4148-454a-b25f-1b7efc9ff56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> QUESTION: What risks are associated with insecure output handling?\n",
      "\n",
      "ANSWER:\n",
      "============================================================\n",
      "Insecure output handling in large language models can lead to several security\n",
      "risks, including:  1. **XSS (Cross-Site Scripting)** and **CSRF (Cross-Site\n",
      "Request Forgery)** 2. **SSRF (Server-Side Request Forgery)** 3. **Privilege\n",
      "Escalation**: LLM-generated content can be used to gain unauthorized access to\n",
      "sensitive data or systems. 4. **Remote Code Execution**: Malicious inputs can be\n",
      "injected into the LLM, allowing attackers to execute arbitrary code on backend\n",
      "systems.  These risks are exacerbated by conditions such as:  - Granting LLM\n",
      "privileges beyond intended use - Indirect prompt injection attacks - Lack of\n",
      "proper output encoding and monitoring - Absence of rate limiting or anomaly\n",
      "detection for LLM usage.\n",
      "============================================================\n",
      "\n",
      "SOURCES:\n",
      "============================================================\n",
      "Number of sources: 3\n",
      "\n",
      "--- Source 1 ---\n",
      "  SECTION: LLM05:2025 Improper Output Handling\n",
      "  HEADING: LLM05:2025 Improper Output Handling\n",
      "\n",
      "  CONTENT:\n",
      "  LLM05:2025 Improper Output Handling Improper Output Handling refers\n",
      "  specifically to insufficient validation, sanitization, and handling of the\n",
      "  outputs generated by large language models before they are passed downstream\n",
      "  to other components and systems. Since LLM-generated content can be\n",
      "  controlled by prompt input, this behavior is similar to providing users\n",
      "  indirect access to additional functionality. Improper Output Handling\n",
      "  differs from Overreliance in that it deals with LLM-generated outputs before\n",
      "  they are passed downstream whereas Overreliance focuses on broader concerns\n",
      "  around overdependence on the accuracy and appropriateness of LLM outputs.\n",
      "  Successful exploitation of an Improper Output Handling vulnerability can\n",
      "  result in XSS and CSRF in web browsers as well as SSRF, privilege\n",
      "  escalation, or remote code execution on backend systems. The following\n",
      "  conditions can increase the impact of this vulnerability: The application\n",
      "  grants the LLM privileges beyond what is intended for end users, enabling\n",
      "  escalation of privileges or remote code execution. •The application is\n",
      "  vulnerable to indirect prompt injection attacks, which could allow an\n",
      "  attacker to gain privileged access to a target user's environment. •3rd\n",
      "  party extensions do not adequately validate inputs. •Lack of proper output\n",
      "  encoding for different contexts (e.g., HTML, JavaScript, SQL) •Insufficient\n",
      "  monitoring and logging of LLM outputs •Absence of rate limiting or anomaly\n",
      "  detection for LLM usage\n",
      "--------------------\n",
      "\n",
      "--- Source 2 ---\n",
      "  SECTION: LLM05:2025 Improper Output Handling\n",
      "  HEADING: LLM05:2025 Improper Output Handling\n",
      "\n",
      "  CONTENT:\n",
      "  LLM05:2025 Improper Output Handling Improper Output Handling refers\n",
      "  specifically to insufficient validation, sanitization, and handling of the\n",
      "  outputs generated by large language models before they are passed downstream\n",
      "  to other components and systems. Since LLM-generated content can be\n",
      "  controlled by prompt input, this behavior is similar to providing users\n",
      "  indirect access to additional functionality. Improper Output Handling\n",
      "  differs from Overreliance in that it deals with LLM-generated outputs before\n",
      "  they are passed downstream whereas Overreliance focuses on broader concerns\n",
      "  around overdependence on the accuracy and appropriateness of LLM outputs.\n",
      "  Successful exploitation of an Improper Output Handling vulnerability can\n",
      "  result in XSS and CSRF in web browsers as well as SSRF, privilege\n",
      "  escalation, or remote code execution on backend systems. The following\n",
      "  conditions can increase the impact of this vulnerability: The application\n",
      "  grants the LLM privileges beyond what is intended for end users, enabling\n",
      "  escalation of privileges or remote code execution. •The application is\n",
      "  vulnerable to indirect prompt injection attacks, which could allow an\n",
      "  attacker to gain privileged access to a target user's environment. •3rd\n",
      "  party extensions do not adequately validate inputs. •Lack of proper output\n",
      "  encoding for different contexts (e.g., HTML, JavaScript, SQL) •Insufficient\n",
      "  monitoring and logging of LLM outputs •Absence of rate limiting or anomaly\n",
      "  detection for LLM usage\n",
      "--------------------\n",
      "\n",
      "--- Source 3 ---\n",
      "  SECTION: LLM05:2025 Improper Output Handling\n",
      "  HEADING: LLM05:2025 Improper Output Handling\n",
      "\n",
      "  CONTENT:\n",
      "  LLM05:2025 Improper Output Handling Improper Output Handling refers\n",
      "  specifically to insufficient validation, sanitization, and handling of the\n",
      "  outputs generated by large language models before they are passed downstream\n",
      "  to other components and systems. Since LLM-generated content can be\n",
      "  controlled by prompt input, this behavior is similar to providing users\n",
      "  indirect access to additional functionality. Improper Output Handling\n",
      "  differs from Overreliance in that it deals with LLM-generated outputs before\n",
      "  they are passed downstream whereas Overreliance focuses on broader concerns\n",
      "  around overdependence on the accuracy and appropriateness of LLM outputs.\n",
      "  Successful exploitation of an Improper Output Handling vulnerability can\n",
      "  result in XSS and CSRF in web browsers as well as SSRF, privilege\n",
      "  escalation, or remote code execution on backend systems. The following\n",
      "  conditions can increase the impact of this vulnerability: The application\n",
      "  grants the LLM privileges beyond what is intended for end users, enabling\n",
      "  escalation of privileges or remote code execution. •The application is\n",
      "  vulnerable to indirect prompt injection attacks, which could allow an\n",
      "  attacker to gain privileged access to a target user's environment. •3rd\n",
      "  party extensions do not adequately validate inputs. •Lack of proper output\n",
      "  encoding for different contexts (e.g., HTML, JavaScript, SQL) •Insufficient\n",
      "  monitoring and logging of LLM outputs •Absence of rate limiting or anomaly\n",
      "  detection for LLM usage\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "res = ask(\"What risks are associated with insecure output handling?\", qa_chain)\n",
    "# res = ask(\"What is data poisoning?\", qa_chain)\n",
    "# res = ask(\"How to mitigate supply chain vulnerabilities\", qa_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b774250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
